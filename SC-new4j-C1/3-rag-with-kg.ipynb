{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load Form 10k Chunk Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Common data processing\n",
    "import json\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "# Langchain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "# from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo4j Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neo4j_version(kg: Neo4jGraph) -> Tuple[int, int, int]:\n",
    "  \"\"\"Return the version of Neo4j as a tuple of ints\"\"\"\n",
    "  version = kg.query(\"CALL dbms.components()\")[0][\"versions\"][0]\n",
    "  if \"aura\" in version:\n",
    "    version_tuple = tuple(map(int, version.split(\"-\")[0].split(\".\"))) + (0,)\n",
    "  else:\n",
    "    version_tuple = tuple(map(int, version.split(\".\")))\n",
    "  return version_tuple\n",
    "\n",
    "def neo4j_create_vector_index(kg: Neo4jGraph, index_name: str, \n",
    "                              for_label: str, on_property: str, \n",
    "                              vector_dimensions: int = 1536,\n",
    "                              similarity_function: str = 'cosine') -> None:\n",
    "  \"\"\"Create a Neo4j index for vector properties\"\"\"\n",
    "  \n",
    "  kg.query(f\"\"\"CREATE VECTOR INDEX `{index_name}` IF NOT EXISTS\n",
    "    FOR (n:{for_label}) ON (n.{on_property}) \n",
    "    OPTIONS {{indexConfig: {{\n",
    "      `vector.dimensions`: {vector_dimensions},\n",
    "      `vector.similarity_function`: '{similarity_function}'\n",
    "    }}}}\"\"\"\n",
    "  )\n",
    "\n",
    "def neo4j_show_properties(kg: Neo4jGraph, on_label: str) -> List:\n",
    "  \"\"\"Introspects the properties of a Neo4j node\n",
    "    by sampling a single node with a specified label\n",
    "    and returning the properties of that node\"\"\"\n",
    "  return kg.query(f\"\"\"\n",
    "    MATCH (n:{on_label}) WITH n LIMIT 1\n",
    "    RETURN apoc.meta.cypher.types(properties(n)) \n",
    "    \"\"\")\n",
    "\n",
    "def neo4j_vector_search(kg: Neo4jGraph, embeddings_model: OpenAIEmbeddings,\n",
    "                        index_name: str, query: str, top_k: int = 10) -> List:\n",
    "  \"\"\"Search for similar nodes using the Neo4j vector index\"\"\"\n",
    "  embedded_query = embeddings_model.embed_query(query)\n",
    "  vector_search = f\"\"\"\n",
    "    CALL db.index.vector.queryNodes($index_name, $top_k, $embedding) yield node, score\n",
    "    RETURN node.text AS result\n",
    "  \"\"\"\n",
    "  similar = kg.query(vector_search, params={'embedding': embedded_query, 'index_name':index_name, 'top_k': top_k})\n",
    "  return similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Set up Neo4j and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load from environment\n",
    "load_dotenv('.env', override=True)\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE') or 'neo4j'\n",
    "\n",
    "# Global constants\n",
    "VECTOR_INDEX_NAME = 'form_10k_chunks'\n",
    "VECTOR_NODE_LABEL = 'Chunk'\n",
    "VECTOR_SOURCE_PROPERTY = 'text'\n",
    "VECTOR_EMBEDDING_PROPERTY = 'textEmbedding'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a knowledge graph using Langchain's Neo4j integration.\n",
    "# This will be used for direct querying of the knowledge graph. \n",
    "kg = Neo4jGraph(\n",
    "    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    ")\n",
    "\n",
    "# OpenAI for creating embeddings\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# Splitting text into chunks using the RecursiveCharacterTextSplitter \n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vector index for the textEmbedding property of Chunk nodes. \n",
    "# Call the index \"form_10k_chunks\" \n",
    "neo4j_create_vector_index(kg, VECTOR_INDEX_NAME, 'Chunk', 'textEmbedding')\n",
    "\n",
    "# Create a uniqueness constraint on the chunkId property of Chunk nodes \n",
    "kg.query('CREATE CONSTRAINT unique_chunk IF NOT EXISTS FOR (n:Chunk) REQUIRE n.chunkId IS UNIQUE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a langchain vector store from the existing Neo4j knowledge graph.\n",
    "vector_store = Neo4jVector.from_existing_graph(\n",
    "    embedding=embeddings_model,\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    index_name=VECTOR_INDEX_NAME,\n",
    "    node_label=VECTOR_NODE_LABEL,\n",
    "    text_node_properties=[VECTOR_SOURCE_PROPERTY],\n",
    "    embedding_node_property=VECTOR_EMBEDDING_PROPERTY,\n",
    ")\n",
    "# Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Create a chatbot Question & Answer chain from the retriever\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    ChatOpenAI(temperature=0), chain_type=\"stuff\", retriever=retriever\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Cypher Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties are the following:\n",
      "\n",
      "Relationship properties are the following:\n",
      "\n",
      "The relationships are the following:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kg.refresh_schema()\n",
    "print(kg.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'count': 0}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run arbitrary Cypher queries by replacing the string \n",
    "# passed below into the kg.query() function\n",
    "kg.query(\"MATCH (n) RETURN count(n) as count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def batches(xs, n=100):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]\n",
    "\n",
    "def make_set_clause(prop_names: ArrayLike, element_name='n', item_name='rec'):\n",
    "    clause_list = []\n",
    "    for prop_name in prop_names:\n",
    "        clause_list.append(f'{element_name}.{prop_name} = {item_name}.{prop_name}')\n",
    "    return 'SET ' + ', '.join(clause_list)\n",
    "\n",
    "\n",
    "def make_node_merge_query(node_key_name: str, node_label: str, cols: ArrayLike):\n",
    "    template = f'''UNWIND $recs AS rec\\nMERGE(n:{node_label} {{{node_key_name}: rec.{node_key_name}}})'''\n",
    "    prop_names = [x for x in cols if x != node_key_name]\n",
    "    if len(prop_names) > 0:\n",
    "        template = template + '\\n' + make_set_clause(prop_names)\n",
    "    return template + '\\nRETURN count(n) AS nodeLoadedCount'\n",
    "\n",
    "def load_nodes(graph: Neo4jGraph, node_df: pd.DataFrame, node_key_col: str, node_label: str, batch_size=1000):\n",
    "    records = node_df.to_dict('records')\n",
    "    print(f'======  loading {node_label} nodes  ======')\n",
    "    total = len(records)\n",
    "    print(f'staging {total:,} records')\n",
    "    query = make_node_merge_query(node_key_col, node_label, node_df.columns.copy())\n",
    "    print(f'\\nUsing This Cypher Query:\\n```\\n{query}\\n```\\n')\n",
    "    cumulative_count = 0\n",
    "    for recs in batches(records, batch_size):\n",
    "        res = graph.query(query, params={'recs': recs})\n",
    "        cumulative_count += res[0].get('nodeLoadedCount')\n",
    "        print(f'Loaded {cumulative_count:,} of {total:,} nodes')\n",
    "\n",
    "\n",
    "def get_and_split_txt_data(file_names: List[str]) -> DataFrame:\n",
    "    doc_data_list = []\n",
    "    for file_name in file_names:\n",
    "        with open(file_name) as f:\n",
    "            f10_k = json.load(f)\n",
    "            for item in ['item1', 'item1a', 'item7', 'item7a']:\n",
    "                #split text data\n",
    "                txt = f10_k[item]\n",
    "                split_txts = text_splitter.split_text(txt)\n",
    "                chunk_seq_id = 0\n",
    "                for split_txt in split_txts:\n",
    "                    form_id = file_name[file_name.rindex('/') + 1:file_name.rindex('.')]\n",
    "                    doc_data_list.append({ 'formId': f'{form_id}',\n",
    "                                           'chunkId': f'{form_id}-{item}-chunk{chunk_seq_id:04d}',\n",
    "                                           'cik': f10_k['cik'],\n",
    "                                           'cusip6': f10_k['cusip6'],\n",
    "                                           'source': f10_k['source'],\n",
    "                                           'f10kItem': item,\n",
    "                                           'chunkSeqId': chunk_seq_id,\n",
    "                                           'text': split_txt})\n",
    "                    chunk_seq_id += 1\n",
    "    return pd.DataFrame(doc_data_list)\n",
    "\n",
    "def add_text_embeddings(df):\n",
    "    count = 0\n",
    "    embeddings = []\n",
    "    for docs in batches(df.text, n=100):\n",
    "        count += len(docs)\n",
    "        print(f'Embedded {count} of {df.shape[0]}')\n",
    "        embeddings.extend(embeddings_model.embed_documents(docs))\n",
    "    df['textEmbedding'] = embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Form 10k documents\n",
    "\n",
    "1. iterate through all the files in the directory\n",
    "2. batch load sets of the files\n",
    "3. for each file, load the content and split the text into chunks\n",
    "4. for each chunk, create a graph Node that includes metadata and the chunk text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing 0:20 of 45 ===\n",
      "Loading and splitting Text Files...\n",
      "Performing Text Embedding...\n",
      "Embedded 100 of 2033\n",
      "Embedded 200 of 2033\n",
      "Embedded 300 of 2033\n",
      "Embedded 400 of 2033\n",
      "Embedded 500 of 2033\n",
      "Embedded 600 of 2033\n",
      "Embedded 700 of 2033\n",
      "Embedded 800 of 2033\n",
      "Embedded 900 of 2033\n",
      "Embedded 1000 of 2033\n",
      "Embedded 1100 of 2033\n",
      "Embedded 1200 of 2033\n",
      "Embedded 1300 of 2033\n",
      "Embedded 1400 of 2033\n",
      "Embedded 1500 of 2033\n",
      "Embedded 1600 of 2033\n",
      "Embedded 1700 of 2033\n",
      "Embedded 1800 of 2033\n",
      "Embedded 1900 of 2033\n",
      "Embedded 2000 of 2033\n",
      "Embedded 2033 of 2033\n",
      "Loading Nodes...\n",
      "======  loading Chunk nodes  ======\n",
      "staging 2,033 records\n",
      "\n",
      "Using This Cypher Query:\n",
      "```\n",
      "UNWIND $recs AS rec\n",
      "MERGE(n:Chunk {chunkId: rec.chunkId})\n",
      "SET n.formId = rec.formId, n.cik = rec.cik, n.cusip6 = rec.cusip6, n.source = rec.source, n.f10kItem = rec.f10kItem, n.chunkSeqId = rec.chunkSeqId, n.text = rec.text\n",
      "RETURN count(n) AS nodeLoadedCount\n",
      "```\n",
      "\n",
      "Loaded 1,000 of 2,033 nodes\n",
      "Loaded 2,000 of 2,033 nodes\n",
      "Loaded 2,033 of 2,033 nodes\n",
      "Done Processing 0:20\n",
      "======  loading Document text embeddings ======\n",
      "staging 2,033 records\n",
      "Set 100 of 2,033 text embeddings\n",
      "Set 200 of 2,033 text embeddings\n",
      "Set 300 of 2,033 text embeddings\n",
      "Set 400 of 2,033 text embeddings\n",
      "Set 500 of 2,033 text embeddings\n",
      "Set 600 of 2,033 text embeddings\n",
      "Set 700 of 2,033 text embeddings\n",
      "Set 800 of 2,033 text embeddings\n",
      "Set 900 of 2,033 text embeddings\n",
      "Set 1,000 of 2,033 text embeddings\n",
      "Set 1,100 of 2,033 text embeddings\n",
      "Set 1,200 of 2,033 text embeddings\n",
      "Set 1,300 of 2,033 text embeddings\n",
      "Set 1,400 of 2,033 text embeddings\n",
      "Set 1,500 of 2,033 text embeddings\n",
      "Set 1,600 of 2,033 text embeddings\n",
      "Set 1,700 of 2,033 text embeddings\n",
      "Set 1,800 of 2,033 text embeddings\n",
      "Set 1,900 of 2,033 text embeddings\n",
      "Set 2,000 of 2,033 text embeddings\n",
      "Set 2,033 of 2,033 text embeddings\n",
      "=== Processing 20:40 of 45 ===\n",
      "Loading and splitting Text Files...\n",
      "Performing Text Embedding...\n",
      "Embedded 100 of 2409\n",
      "Embedded 200 of 2409\n",
      "Embedded 300 of 2409\n",
      "Embedded 400 of 2409\n",
      "Embedded 500 of 2409\n",
      "Embedded 600 of 2409\n",
      "Embedded 700 of 2409\n",
      "Embedded 800 of 2409\n",
      "Embedded 900 of 2409\n",
      "Embedded 1000 of 2409\n",
      "Embedded 1100 of 2409\n",
      "Embedded 1200 of 2409\n",
      "Embedded 1300 of 2409\n",
      "Embedded 1400 of 2409\n",
      "Embedded 1500 of 2409\n",
      "Embedded 1600 of 2409\n",
      "Embedded 1700 of 2409\n",
      "Embedded 1800 of 2409\n",
      "Embedded 1900 of 2409\n",
      "Embedded 2000 of 2409\n",
      "Embedded 2100 of 2409\n",
      "Embedded 2200 of 2409\n",
      "Embedded 2300 of 2409\n",
      "Embedded 2400 of 2409\n",
      "Embedded 2409 of 2409\n",
      "Loading Nodes...\n",
      "======  loading Chunk nodes  ======\n",
      "staging 2,409 records\n",
      "\n",
      "Using This Cypher Query:\n",
      "```\n",
      "UNWIND $recs AS rec\n",
      "MERGE(n:Chunk {chunkId: rec.chunkId})\n",
      "SET n.formId = rec.formId, n.cik = rec.cik, n.cusip6 = rec.cusip6, n.source = rec.source, n.f10kItem = rec.f10kItem, n.chunkSeqId = rec.chunkSeqId, n.text = rec.text\n",
      "RETURN count(n) AS nodeLoadedCount\n",
      "```\n",
      "\n",
      "Loaded 1,000 of 2,409 nodes\n",
      "Loaded 2,000 of 2,409 nodes\n",
      "Loaded 2,409 of 2,409 nodes\n",
      "Done Processing 20:40\n",
      "======  loading Document text embeddings ======\n",
      "staging 2,409 records\n",
      "Set 100 of 2,409 text embeddings\n",
      "Set 200 of 2,409 text embeddings\n",
      "Set 300 of 2,409 text embeddings\n",
      "Set 400 of 2,409 text embeddings\n",
      "Set 500 of 2,409 text embeddings\n",
      "Set 600 of 2,409 text embeddings\n",
      "Set 700 of 2,409 text embeddings\n",
      "Set 800 of 2,409 text embeddings\n",
      "Set 900 of 2,409 text embeddings\n",
      "Set 1,000 of 2,409 text embeddings\n",
      "Set 1,100 of 2,409 text embeddings\n",
      "Set 1,200 of 2,409 text embeddings\n",
      "Set 1,300 of 2,409 text embeddings\n",
      "Set 1,400 of 2,409 text embeddings\n",
      "Set 1,500 of 2,409 text embeddings\n",
      "Set 1,600 of 2,409 text embeddings\n",
      "Set 1,700 of 2,409 text embeddings\n",
      "Set 1,800 of 2,409 text embeddings\n",
      "Set 1,900 of 2,409 text embeddings\n",
      "Set 2,000 of 2,409 text embeddings\n",
      "Set 2,100 of 2,409 text embeddings\n",
      "Set 2,200 of 2,409 text embeddings\n",
      "Set 2,300 of 2,409 text embeddings\n",
      "Set 2,400 of 2,409 text embeddings\n",
      "Set 2,409 of 2,409 text embeddings\n",
      "=== Processing 40:45 of 45 ===\n",
      "Loading and splitting Text Files...\n",
      "Performing Text Embedding...\n",
      "Embedded 100 of 598\n",
      "Embedded 200 of 598\n",
      "Embedded 300 of 598\n",
      "Embedded 400 of 598\n",
      "Embedded 500 of 598\n",
      "Embedded 598 of 598\n",
      "Loading Nodes...\n",
      "======  loading Chunk nodes  ======\n",
      "staging 598 records\n",
      "\n",
      "Using This Cypher Query:\n",
      "```\n",
      "UNWIND $recs AS rec\n",
      "MERGE(n:Chunk {chunkId: rec.chunkId})\n",
      "SET n.formId = rec.formId, n.cik = rec.cik, n.cusip6 = rec.cusip6, n.source = rec.source, n.f10kItem = rec.f10kItem, n.chunkSeqId = rec.chunkSeqId, n.text = rec.text\n",
      "RETURN count(n) AS nodeLoadedCount\n",
      "```\n",
      "\n",
      "Loaded 598 of 598 nodes\n",
      "Done Processing 40:45\n",
      "======  loading Document text embeddings ======\n",
      "staging 598 records\n",
      "Set 100 of 598 text embeddings\n",
      "Set 200 of 598 text embeddings\n",
      "Set 300 of 598 text embeddings\n",
      "Set 400 of 598 text embeddings\n",
      "Set 500 of 598 text embeddings\n",
      "Set 598 of 598 text embeddings\n",
      "CPU times: user 31.6 s, sys: 845 ms, total: 32.4 s\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "all_file_names = ['./data/form10k/' + x for x in os.listdir('./data/form10k/')]\n",
    "counter = 0\n",
    "for file_names in batches(all_file_names, 20):\n",
    "    counter += len(file_names)\n",
    "    print(f'=== Processing {counter-len(file_names)}:{counter} of {len(all_file_names)} ===')\n",
    "    # get and split text data\n",
    "    print('Loading and splitting Text Files...')\n",
    "    doc_df = get_and_split_txt_data(file_names)\n",
    "    # perform text embedding\n",
    "    print('Performing Text Embedding...')\n",
    "    add_text_embeddings(doc_df)\n",
    "    #load nodes\n",
    "    print('Loading Nodes...')\n",
    "    load_nodes(kg, doc_df.drop(columns='textEmbedding'), 'chunkId', 'Chunk')\n",
    "    print(f'Done Processing {counter-len(file_names)}:{counter}')\n",
    "\n",
    "    # Merge text embeddings using set vector property\n",
    "    records = doc_df[['chunkId', 'textEmbedding']].to_dict('records')\n",
    "    print(f'======  loading Document text embeddings ======')\n",
    "    total = len(records)\n",
    "    print(f'staging {total:,} records')\n",
    "    cumulative_count = 0\n",
    "    for recs in batches(records, n=100):\n",
    "        res = kg.query('''\n",
    "        UNWIND $recs AS rec\n",
    "        MATCH(n:Chunk {chunkId: rec.chunkId})\n",
    "        CALL db.create.setNodeVectorProperty(n, \"textEmbedding\", rec.textEmbedding)\n",
    "        RETURN count(n) AS propertySetCount\n",
    "        ''', params={'recs': recs})\n",
    "        cumulative_count += res[0].get('propertySetCount')\n",
    "        print(f'Set {cumulative_count:,} of {total:,} text embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties are the following:\n",
      "Chunk {f10kItem: STRING, chunkSeqId: INTEGER, textEmbedding: LIST, chunkId: STRING, cik: STRING, cusip6: STRING, text: STRING, formId: STRING, source: STRING}\n",
      "Relationship properties are the following:\n",
      "\n",
      "The relationships are the following:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kg.refresh_schema()\n",
    "print(kg.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Who makes GPUs?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'result': 'The growth in demand for associative processing computing solutions is being driven by the increasing market adoption and usage of graphics processing unit (“GPU”) and CPU farms for AI processing of large data collections, including parallel computing in scientific research.  However, the large-scale usage of GPU and CPU farms for AI processing of data is demonstrating the limits of GPU and CPU processing speeds and resulting in ever higher energy consumption. The amounts of data being processed, which is coming from increasing numbers of users and continuously increasing amounts of collected data, has resulted in efforts to split and store the processed data among multiple databases, through a process called sharding.  Sharding substantially increases processing costs and worsens the power consumption factors associated with processing so much data.  As the environmental impacts of data processing are becoming increasingly important, and complex workloads are migrating to edge computing for real-time applications, it is becoming increasingly difficult to achieve market demands for low power, smaller footprints and faster results.'},\n",
       " {'result': 'Our principal competitors include NVIDIA Corporation and Intel Corporation for our in-place associative computing solutions and \\nInfineon Technologies AG\\n, Integrated Silicon Solution and REC for our SRAM products.  We expect additional competitors to enter the associative computing market as well. While some of our competitors \\n12'},\n",
       " {'result': 'All of our manufactured wafers, including wafers for our APU product, are tested for electrical compliance and most are packaged at Advanced Semiconductor Engineering (“ASE”) which is located in Taiwan. Wistron Neweb Corporation in Taiwan manufactures the boards for our APU product line. Our test procedures require that all of our products be subjected to accelerated burn-in and extensive functional electrical testing which is performed at our Taiwan and U.S. test facilities. Our radiation-hardened products are assembled and tested at Silicon Turnkey Solutions Inc., located near our Sunnyvale, California headquarters facility.\\nResearch and Development\\nWe have devoted substantial resources in the last seven years on the development of our APU products. Our research and development staff includes engineering professionals with extensive experience in the areas of high-speed circuit design, including APU design, as well as SRAM design and systems level networking and telecommunications equipment design. Additionally, we have assembled a team of software development experts in Israel needed for the development of the various levels of software required in the use of our APU products. The design process for our products is complex. As a result, we have made substantial investments in computer-aided design and engineering resources to manage our design process.\\nCompetition\\nOur existing and potential competitors include many large domestic and international companies, some of which have substantially greater resources, offer other types of memory and/or non-memory technologies and may have longer standing relationships with OEM customers than we do. Unlike us, some of our principal competitors maintain their own semiconductor fabs, which may, at times, provide them with capacity, cost and technical advantages.\\nOur principal competitors include NVIDIA Corporation and Intel Corporation for our in-place associative computing solutions and \\nInfineon Technologies AG'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector search using our utility function\n",
    "neo4j_vector_search(kg, embeddings_model, VECTOR_INDEX_NAME, question, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Score:  0.9043025970458984\n",
      "\n",
      "text: The growth in demand for associative processing computing solutions is being driven by the increasing market adoption and usage of graphics processing unit (“GPU”) and CPU farms for AI processing of large data collections, including parallel computing in scientific research.  However, the large-scale usage of GPU and CPU farms for AI processing of data is demonstrating the limits of GPU and CPU processing speeds and resulting in ever higher energy consumption. The amounts of data being processed, which is coming from increasing numbers of users and continuously increasing amounts of collected data, has resulted in efforts to split and store the processed data among multiple databases, through a process called sharding.  Sharding substantially increases processing costs and worsens the power consumption factors associated with processing so much data.  As the environmental impacts of data processing are becoming increasingly important, and complex workloads are migrating to edge computing for real-time applications, it is becoming increasingly difficult to achieve market demands for low power, smaller footprints and faster results.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.9009170532226562\n",
      "\n",
      "text: Our principal competitors include NVIDIA Corporation and Intel Corporation for our in-place associative computing solutions and \n",
      "Infineon Technologies AG\n",
      ", Integrated Silicon Solution and REC for our SRAM products.  We expect additional competitors to enter the associative computing market as well. While some of our competitors \n",
      "12\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.8966693878173828\n",
      "\n",
      "text: All of our manufactured wafers, including wafers for our APU product, are tested for electrical compliance and most are packaged at Advanced Semiconductor Engineering (“ASE”) which is located in Taiwan. Wistron Neweb Corporation in Taiwan manufactures the boards for our APU product line. Our test procedures require that all of our products be subjected to accelerated burn-in and extensive functional electrical testing which is performed at our Taiwan and U.S. test facilities. Our radiation-hardened products are assembled and tested at Silicon Turnkey Solutions Inc., located near our Sunnyvale, California headquarters facility.\n",
      "Research and Development\n",
      "We have devoted substantial resources in the last seven years on the development of our APU products. Our research and development staff includes engineering professionals with extensive experience in the areas of high-speed circuit design, including APU design, as well as SRAM design and systems level networking and telecommunications equipment design. Additionally, we have assembled a team of software development experts in Israel needed for the development of the various levels of software required in the use of our APU products. The design process for our products is complex. As a result, we have made substantial investments in computer-aided design and engineering resources to manage our design process.\n",
      "Competition\n",
      "Our existing and potential competitors include many large domestic and international companies, some of which have substantially greater resources, offer other types of memory and/or non-memory technologies and may have longer standing relationships with OEM customers than we do. Unlike us, some of our principal competitors maintain their own semiconductor fabs, which may, at times, provide them with capacity, cost and technical advantages.\n",
      "Our principal competitors include NVIDIA Corporation and Intel Corporation for our in-place associative computing solutions and \n",
      "Infineon Technologies AG\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Vector search using the langchain vector store\n",
    "docs_with_score = vector_store.similarity_search_with_score(question, k=3)\n",
    "\n",
    "for doc, score in docs_with_score:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\\ntext: The growth in demand for associative processing computing solutions is being driven by the increasing market adoption and usage of graphics processing unit (“GPU”) and CPU farms for AI processing of large data collections, including parallel computing in scientific research.  However, the large-scale usage of GPU and CPU farms for AI processing of data is demonstrating the limits of GPU and CPU processing speeds and resulting in ever higher energy consumption. The amounts of data being processed, which is coming from increasing numbers of users and continuously increasing amounts of collected data, has resulted in efforts to split and store the processed data among multiple databases, through a process called sharding.  Sharding substantially increases processing costs and worsens the power consumption factors associated with processing so much data.  As the environmental impacts of data processing are becoming increasingly important, and complex workloads are migrating to edge computing for real-time applications, it is becoming increasingly difficult to achieve market demands for low power, smaller footprints and faster results.', metadata={'cik': '1126741', 'source': 'https://www.sec.gov/Archives/edgar/data/1126741/000155837023011516/0001558370-23-011516-index.htm', 'formId': '0001558370-23-011516', 'f10kItem': 'item1', 'chunkId': '0001558370-23-011516-item1-chunk0007', 'cusip6': '36241U', 'chunkSeqId': 7})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector search using the langchain retriever over the Neo4j vector store\n",
    "retriever.get_relevant_documents(question)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'NVIDIA Corporation and Intel Corporation are competitors in the GPU market.\\n',\n",
       " 'sources': 'https://www.sec.gov/Archives/edgar/data/1126741/000155837023011516/0001558370-23-011516-index.htm'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain(\n",
    "    {\"question\": question},\n",
    "    return_only_outputs=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
